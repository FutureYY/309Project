{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a633d03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m when, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m, col, \u001b[38;5;28mround\u001b[39m, lower, trim, countDistinct, count, \u001b[38;5;28msum\u001b[39m, month, hour\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Window\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# importing libraries needed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when, min, max, col, round, lower, trim, countDistinct, count, sum, month, hour\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.sql.functions import sum as spark_sum, col, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704ea3d",
   "metadata": {},
   "source": [
    "# Starting PySpark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a05c753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'spark-3.5.6-bin-hadoop3-scala2.13.tgz'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.5.6-bin-hadoop3-scala2.13\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jiawe\\OneDrive\\Documents\\GitHub\\309Project\\.venv\\Lib\\site-packages\\findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     py4j \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_python\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpy4j-*.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/spark-3.5.6-bin-hadoop3-scala2.13\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mfindspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiawe\\OneDrive\\Documents\\GitHub\\309Project\\.venv\\Lib\\site-packages\\findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    159\u001b[0m         py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find py4j in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    163\u001b[0m                 spark_python\n\u001b[0;32m    164\u001b[0m             )\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys_path \u001b[38;5;241m=\u001b[39m [spark_python, py4j]\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.5.6-bin-hadoop3-scala2.13\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3-scala2.13.tgz\n",
    "!tar xf spark-3.5.6-bin-hadoop3-scala2.13.tgz\n",
    "!pip install -q findspark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.6-bin-hadoop3-scala2.13\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b89a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.master\", 'local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc3bca",
   "metadata": {},
   "source": [
    "# Introduction to EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db88c5",
   "metadata": {},
   "source": [
    "Objectives for this EDA:\n",
    "\n",
    "1. Understand and doing basic cleaning on the datasets given\n",
    "- Find missing values, duplicates or data that have inconsistent formats\n",
    "- Identify and handle outliers\n",
    "- Checking for invalid timestamps (e.g. delivery occurs before purchase date)\n",
    "\n",
    "2. Distribution of data and relationship\n",
    "- Most commmon product categories, payment methods, purchase hour\n",
    "\n",
    "3. Customer Behaviour Analysis\n",
    "- How many repeat buyers exist?\n",
    "- Average time between orders for repeat buyers\n",
    "- Preference of repeat buyers (What are they repurchasing)\n",
    "\n",
    "4. Payment analysis\n",
    "- Does customers who make paymetns with vouchers behave diffferntly?\n",
    "- Is there a relationship between high installments (high installment value/high amount of installments made) and low repurchases?\n",
    "\n",
    "5. Delivery pattern\n",
    "- How long does delivery take?\n",
    "- Will long delivery duration cause bad reviews or lower chances of repeat orders?\n",
    "- Distance between customer and seller\n",
    "\n",
    "6. Review score\n",
    "- Are review scores linked to repeat purchases?\n",
    "\n",
    "7. Product category\n",
    "- Which product categories are the most popular?\n",
    "- Group categories which are not popular under 'others'\n",
    "- Do one-hot encoding on product categories\n",
    "\n",
    "Final EDA Goal:\n",
    "\n",
    "To prepare, clean, insightful dataset with features that will be helpful in helping the model to find repeat buyers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55094be",
   "metadata": {},
   "source": [
    "### 1. Understanding and doing basic cleaning on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# christel's code will be in/under this markdown cell\n",
    "\n",
    "# Find missing values, duplicates or data that have inconsistent formats\n",
    "# Identify and handle outliers\n",
    "# Checking for invalid timestamps (e.g. delivery occurs before purchase date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad27e0",
   "metadata": {},
   "source": [
    "### 2. Distribution of data and relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d809657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most commmon product categories, payment methods, purchase hour\n",
    "\n",
    "# most common cities \n",
    "# most common purchase hour\n",
    "# most common product categories\n",
    "# most common payment type\n",
    "# distribution of no. of installments made/value\n",
    "# distribution of review score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be449f2",
   "metadata": {},
   "source": [
    "### 3. Customer Behaviour Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af96d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many repeat buyers exist?\n",
    "# Average time between orders for repeat buyers\n",
    "# Preference of repeat buyers (What are they repurchasing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96044db4",
   "metadata": {},
   "source": [
    "### 4. Payment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f52cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does customers who make paymetns with vouchers behave diffferntly?\n",
    "# Is there a relationship between high installments (high installment value/high amount of installments made) and low repurchases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e348b5",
   "metadata": {},
   "source": [
    "### 5. Delivery pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long does delivery take?\n",
    "# Will long delivery duration cause bad reviews or lower chances of repeat orders?\n",
    "# Distance between customer and seller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474800ca",
   "metadata": {},
   "source": [
    "### 6. Review score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58111f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are review scores linked to repeat purchases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a067c",
   "metadata": {},
   "source": [
    "### 7. Product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959480c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which product categories are the most popular?\n",
    "# Group categories which are not popular under 'others'\n",
    "# Do one-hot encoding on product categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
